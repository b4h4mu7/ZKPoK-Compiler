
\subsection{Framework Overview}
\label{frameworkoverview}
We propose a custom ZPKP compiler framework that takes a protocol
implementation in PIL as generated by the CACE ZKPK compiler and
produces GEZEL or C+GMP code. This allows exploring both ends of the
hardware-software co-design spectrum.
Figure~\ref{fig:custom_framework_workflow} gives an overview of the
custom framework.

\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth,level distance=0.85cm, font=\tiny]
    \tikzstyle{edge from parent}=[draw,->] \tikzset{every leaf
      node/.style={anchor=center}}

    \Tree [.\node[language](psl){Protocol Specification \\ Language (PSL)};
      [.\node[compiler](pc){Protocol Compiler};
        [.\node[language](pil){Protocol Implementation \\ Language (PIL)};
          [.\node[compiler,added](llvm){LLVM};
            \node[language,added](asm){C+GMP};
            \node[language,added](gezel){GEZEL};
          ]
          [.\node[compiler](c){C}; \node[language](code){Code};]
          [.\node[compiler](latex){\LaTeX}; \node[language](doc){Documentation};]
        ]
      ]
    ]

    \node[compiler] (pvt)         [right=of pc,anchor=west]          {Protocol Verification \\ Toolbox (PVT)}
    child {node[language] {Proof of \\ Soundness}};

    \node[compiler] (sigma) [left=of pil.north west,anchor=center] {$\Sigma 2 N I Z K$};
    \node[compiler] (cost) [left=of pil.south west,anchor=center] {Costs};

    \draw[<->] (sigma) -- (pil);
    \draw[<->] (cost) -- (pil);

    \draw[->] (psl) -- (pvt);
    \draw[->] (pil) -- (pvt);
  \end{tikzpicture}
  \caption{Custom framework (extensions to CACE Zero Knowledge
    Compiler highlighted)}
  \label{fig:custom_framework_workflow}
\end{figure}

The generated code from the CACE ZKPK compiler is linked with a
supporting library that is not suitable for small-embedded
devices. The supporting library uses GMP but adds additional
complexity atop. Additionally, our extensions to PIL make it
incompatible with CACE ZKPK compiler.

In order to allow compatibility and improve performance we provide a
software back-end via GMP as well. In our case, there is no added
complexity or semantics. Additionally, we note that these basic GMP
semantics allow us to emulate a target supporting arbitrary-precision
arithmetic.

LLVM was chosen as the compiler framework to transform ZKPK
implementations in PIL into implementations in the desired
languages. The worfklow is depicted in Figure
\ref{fig:custom_llvm_workflow}. The starting point is a ZKPK
implementation in PIL. This implementation is processed by a PIL
Front-end, described in Section~\ref{pilfrontend}, which generates
LLVM IR code. The implementation in LLVM IR can then be used as a
starting point for multiple targets. The LLVM framework provides
back-ends for architectures such as x86, PowerPC and
ARM\footnote{Unfortunately, we cannot use these targets as they do not
  support arbitrary precision arithmetic}. Section~\ref{gezelbackend}
describes the C+GMP and GEZEL back-ends, along with some extensions to GEZEL. Extensions to PIL are 
described in Section~\ref{extensionsPIL}.
\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \tikzstyle{lang}=[rectangle,draw=black,thin,font=\tiny,inner
    sep=0pt, align=center,minimum width=2cm,minimum height=2.2em]

    \tikzstyle{txt}=[font=\tiny]

    \node[lang](llvm_opt){LLVM \\ Optimizer};

    \node[lang,added](gmp_back)[right=1 cm of llvm_opt]{Custom \\ GMP Backend};
    \node[txt](gmp)[right=of gmp_back]{C+GMP code};

    \node[lang,added](gezel_back)[below=of gmp_back]{Custom \\ GEZEL Backend};
    \node[txt](gezel)[right=of gezel_back]{GEZEL};

    \node[lang,added](llvm_storeloadopt)[below=1 cm of llvm_opt]{Store/Load \\ Optimizer};

    \node[lang,added](pil_front)[left=1 cm of llvm_opt]{PIL \\ Frontend};
    \node[txt](pil)[left=of pil_front]{PIL};

    \draw[->] (pil_front.east) -- (llvm_opt.west);

    \draw[->] (pil) -- (pil_front);

    \draw[->] (llvm_opt.east) -- (gezel_back.west);
    \draw[->] (llvm_opt.east) -- (gmp_back.west);

    \draw[->] (gezel_back) -- (gezel);
    \draw[->] (gmp_back) -- (gmp);

    \draw[<->] (llvm_opt) -- (llvm_storeloadopt);
  \end{tikzpicture}
  \caption{LLVM custom workflow (changes highlighted)}
  \label{fig:custom_llvm_workflow}
\end{figure}


%\subsection{Extensions to CACE Zero-Knowledge Proofs Compiler}
%\label{extensions}

\subsection{Extensions to PIL}
\label{extensionsPIL}

\paragraph{Multiple Blocks.}
PIL specifies Prover and Verifier using respective blocks. A block can
comprise several functions, representing each of the protocol
rounds. The execution order of these rounds should also be
specified. Rounds are executed sequentially.

Besides Prover and Verifier, only an additional Common block, which
contains declarations visible to all the blocks, can be
specified. This makes it impossible to implement a multiparty
protocol, such as the DAA~\cite{DBLP:conf/ccs/BrickellCC04}. We allow
multiple blocks via a simple relaxation of the rules.

\paragraph{Global Variable Access.}
To assure the integrity of the variables in PIL, we have constrained
the language such that global variables can only be modified by the
executing round. This allows the global variable to be backed by a
local variable, which represents the value of the global variable for
the entire duration of the round. Only the last modification gets
written back to the global variable. This has the effect of converting
the CFG to a trivial one with a single node. Previously, the
instructions dominated by the value of the load instruction could not
be executed before the load instruction completed. Without the load
instruction, the ordering can again be arbitrary given the DFG is
satisfied.

The global variables are usually stored in a higher-latency, slower
access storage (register or cache or main memory) so this constraint
will actually improve the performance by accessing the global only
once per read or write.

By introducing this constraint we also protect the implementations
from side-channel attacks. The attacker cannot easily deduce the
location of the storage location of the global variable before the
global variable is actually read. Since the read and write need to
happen only once per round, the time available to launch an attack is
very short.

\paragraph{Compile Time/Constant Expressions.} PIL does not allow
constant expressions as parameters of variables. For example, the following code to declare an integer $f$ of length $l_f + l_{phi} + l_H$ is not allowed.
\begin{lstlisting}[language=PIL]
Common (
  Z l_f = 160;
  Z l_phi = 80;
  Z l_H = 160;
) {}
Smartcard (
  Int(l_f + l_phi + l_H) f
) {}
\end{lstlisting}
Without constant expressions, one would need to recompute the values
manually and re-enter them every time a modification is needed, which is prone to errors. Therefore, we provide PIL with this feature.

\paragraph{Type inference.}
Type inference allows to determine the resulting
type of a certain expression and can also be used to omit
a type declaration. The following example illustrates this:
\begin{lstlisting}[language=PIL]
Zmod*(p) b;
x := Random(Int(80));
a := b^x;
\end{lstlisting}
The type of x can be inferred as Int(80) since
the Random function can only return a random value of the provided
type. As for variable a, since the operation of exponentiation is defined as
applying the multiplication operation many times, the type is Zmod*(p). A similar argument holds when multiplying an element
of the additive modular residue group with an integer. Consequently,
type inference is well defined for any acceptable operation in PIL.

\subsection{PIL Front-end}
\label{pilfrontend}

The purpose of the PIL Front-end is to transform PIL code into LLVM IR code. The PIL Front-end flow is depicted in Figure~\ref{fig:pil_frontend_flow}. First, the protocol implementation in PIL is read by the Lexer, which produces input for the Parser. Next, the Parser reads this input and generates an abstract syntax tree (AST). Finally, the AST is given as input to the Codegen tree-walker, which outputs the protocol implementation in LLVM IR.

\begin{figure}[hb!]
  \begin{tikzpicture}[>=stealth]
    \node[language] (pil) {PIL};
    \node[compiler] (lexer) [right=of pil] {Lexer};
    \node[compiler] (parser) [right=of lexer] {Parser};
    \node[language] (ast) [right=of parser] {AST};
    \node[compiler] (codegen) [right=of ast] {Codegen};
    \node[language] (llvm_ir) [right=of codegen] {LLVM IR};

    \draw[->] (pil) -- (lexer);
    \draw[->] (lexer) -- (parser);
    \draw[->] (parser) -- (ast);
    \draw[->] (ast) -- (codegen);
    \draw[->] (codegen) -- (llvm_ir);
  \end{tikzpicture}
  \caption{PIL frontend flow}
  \label{fig:pil_frontend_flow}
\end{figure}

% Both the lexer grammar and the parser grammar are specified in the file pil.g, while the tree-walker and the code generator are
% specified in the file codegen.g. As shown in Figure~\ref{fig:pil_parser_codegen}, ANTLR is used to generate the lexer, the parser and the Codegen tree-walker.
% \begin{figure}[hb!]
%   \centering
%   \subfloat{
%   \begin{tikzpicture}[>=stealth]
%     \tikzstyle{edge from parent}=[draw,->]

%     \Tree[.\node[language](parser_g){pil.g};
%       [.\node[compiler](antlr){ANTLR};
%         [.\node[compiler](lexer){Lexer};]
%         [.\node[compiler](parser){Parser};]
%       ]
%     ]
%   \end{tikzpicture}
%   } \qquad
%   \subfloat{
%   \begin{tikzpicture}[>=stealth]
%     \tikzstyle{edge from parent}=[draw,->]

%     \Tree[.\node[language](tree_g){codegen.g};
%       [.\node[compiler](antlr2){ANTLR};
%         [.\node[compiler](walker){Codegen};]
%       ]
%     ]
%   \end{tikzpicture}
%   }
%   \caption{Lexer, parser and tree walker generation}
%   \label{fig:pil_parser_codegen}
% \end{figure}

The code generation process generates one LLVM module per PIL block.
Every other block except the Common block gets a Common block linked
in. This process is depicted in Figure \ref{fig:linker}.
\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \node[language] (block) {Block};
    \node[compiler] (linker) [right=of block] {Linker};
    \node[language] (common) [above=of linker] {Common};
    \node[language] (module) [right=of linker] {Module};

    \draw[->] (block) -- (linker);
    \draw[->] (common) -- (linker);
    \draw[->] (linker) -- (module);
  \end{tikzpicture}
  \caption{Linker}
  \label{fig:linker}
\end{figure}

As result of the transformation, PIL private parameters and global
variables are transformed into LLVM IR global variables, the private
parameters being constant in this case. Each PIL function of a block
is transformed into an LLVM IR function whose input and output
arguments are transformed as such. LLVM's constant folder is used to
evaluate constant time expressions in order to simplify them into a
constant value.

There are two methods to equip LLVM IR with the group types and
arithmetic provided by PIL: either include group types in the core
LLVM files or use the existing IntegerType of LLVM IR and make the
compiler do the extra work. The former allows to preserve the
information about the modular residue groups to the lowest level,
which maintains the security and verifiability of the implementation
to the lowest possible level. However, it requires changes at the core
of the complex LLVM framework, which is error-prone, and it hinders
tracking new LLVM versions if those changes are not integrated into
the main LLVM project.

Therefore, the latter approach was chosen. The group types are backed
by an IntegerType of the appropriate length in the LLVM IR, while type
inference is used to deduce the resulting type of an
operation. Modular operations are transformed into appropriate 3
argument operations with the first 2 arguments being the integer
operands and the third being the modulus.

As result, compatibility with existing LLVM applications is
maintained. The resulting compiler is more complex because it needs to
track types and apply the appropriate operations in the case of group
types. Additionally, if an architecture supports group types (e.g., an
automatic verifier), the workload of its corresponding back-end
increases since now it has to regenerate the lost information
regarding group types.

\subsection{Target Back-ends}
\paragraph{Extensions to GEZEL.}
\label{extensionsGEZEL}

In order to implement modular arithmetic in GEZEL, it is necessary to
follow a two step approach. First, the operation is executed over the
integers, and, second, the modular reduction of the result is
calculated. This approach is valid to implement addition, subtraction
and multiplication, but it cannot be used to support modular
exponentiations.

There are two methods to solve this lack of support: either simulate a
modular exponentiation via implementing it in GEZEL, or extend GEZEL
so that it provides support for modular exponentiation. The latter
approach was taken since it was deemed necessary to enrich the
language with this operation.

\paragraph{GEZEL Back-end.}
\label{gezelbackend}
The GEZEL back-end designs a purely hardware, purely intra-round
combinatorial design. Our PIL semantics allow global variables to be
read at most once and written at most once without any imposed
order. Since there is no enforced order, this allows us to schedule
all the operations in a single clock-cycle.

\paragraph{GMP Back-end.}
\label{gmpbackend}
All of the architectures LLVM supports lack multi-precision arithmetic
support.  This multi-precision support has to come from the software
side via a support library. GMP was chosen as it was deemed the best
option given stability, licensing and performance. The performance
results are due to the efficiency of the GMP library. In this manner,
we take after the CACE ZKPK Compiler.

%%% Local Variables:
%%% TeX-PDF-mode: t
%%% TeX-master: "main"
%%% End:
