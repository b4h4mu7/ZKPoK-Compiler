\chapter{Custom Framework}

This chapter aims to present the custom framework that was developed
within the course of this thesis. It starts with the motivation of
domain specific languages then gives the reasoning to extend the CACE
Project Zero Knowledge Compiler. Figure
\ref{fig:custom_framework_workflow} gives an overview of the custom
extensions. They are presented in detail in Section
\ref{sec:cace_extensions}. All of targets except for GEZEL come from
LLVM so the GEZEL target will also be covered.

\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth,level distance=1.5cm, font=\tiny]
    \tikzstyle{edge from parent}=[draw,->] \tikzset{every leaf
      node/.style={anchor=center}}

    \Tree [.\node[language](psl){Protocol \\ Specification \\ Language (PSL)};
      [.\node[compiler](pc){Protocol \\ Compiler};
        [.\node[language](pil){Protocol \\ Implementation \\ Language (PIL)};
          [.\node[compiler,added](llvm){LLVM};
            \node[language,added](asm){x86 \\ x86-64 \\ MIPS \\ARM};
            \node[language,added](gezel){GEZEL \\ VHDL \\ Verilog};
          ]
          [.\node[compiler](c){C}; \node[language](code){Code};]
          [.\node[compiler](latex){\LaTeX}; \node[language](doc){Documentation};]
        ]
      ]
    ]

    \node[compiler] (pvt)         [right=of pc,anchor=west]          {Protocol \\ Verification \\ Toolbox}
    child {node[language] {Proof of \\ Soundness}};

    \node[compiler] (sigma) [left=of pil.north west,anchor=center] {$\Sigma 2 N I Z K$};
    \node[compiler] (cost) [left=of pil.south west,anchor=center] {Costs};

    \draw[<->] (sigma) -- (pil);
    \draw[<->] (cost) -- (pil);

    \draw[->] (psl) -- (pvt);
    \draw[->] (pil) -- (pvt);
  \end{tikzpicture}
  \caption{Custom framework (extensions to CACE Zero Knowledge
    Compiler highlighted)}
  \label{fig:custom_framework_workflow}
\end{figure}

\section{Motivation}

\subsection{Domain Specific Language}

C is not a very safe language for cryptography applications. This
can be seen already from the goal of C which was to design a
``portable'' assembler. The design choice was not to sacrifice
efficiency so some things are intentionally left undefined or
unspecified in the standard. It is left to the end compilers to
implement it as they choose. The usual choice is just to implement in
the most efficient way for the target platform. For example, the
following code uses this unspecified behavior:

\begin{lstlisting}[language=C]
#include <stdio.h>

int b(void) { puts("3"); return 3; }
int c(void) { puts("4"); return 4; }

int main(void)
{
  int a = b() + c();
  printf("%d\n", a);

  return 0;
}
\end{lstlisting}

The evaluation order is unspecified so depending on the compiler and
the platform, the output may be 3, 4, 7 or 4, 3, 7. The crypto world
should not have undefined nor unspecified behaviors so this is why a
domain specific language is a key point of this thesis.

A domain specific languages hides all the operations that are deemed
not important and allows an abstract overview of the operations. This

\subsection{Extending CACE Zero Knowledge Compiler}

The C code generated by CACE uses GNU GMP, which is a multi-precision
arithmetic library. This library is tailored for desktop computers and
is not well suited for small embedded devices. Separation of the
supporting library allows plugging in of a custom multi-precision
library but this is impractical. There are various devices available
and writing a multi-precision library for each of them is error prone
and inefficient. 

The fact that the CACE Project Zero Knowledge Compiler already uses
its own domain specific language makes it a useful candidate for
extension. It is possible to use LLVM as a low level language and
assure portability and type strictness.

ZKPDL also has its own domain specific language. However, the approach
of creating an interpreter is considered too inefficient for embedded
devices.

As the CACE project is deemed more suited for the task, the custom
framework will extend CACE. LLVM already allows for
\begin{itemize}
\item Interpreted code - by using its interpreter
\item JIT (Just In Time) compiled code - by compiling the code at runtime
\item Compiled code - by allowing to code to be compiled later on
\end{itemize}
Also, since the intermediate form of LLVM is of the SSA (Static Single
Assignment) form, it is believed that more aggressive optimizations
are possible. Since the DFG (Data Flow Graph) can be easily extracted
from the SSA form, hardware realizations are also made possible.

\section{Extensions to GEZEL}

\subsection{Terminal communication ipblocks}

A modification to GEZEL was needed to allow interactivity. A terminal
ipblock was made during the course of this thesis. This ipblock allows
connecting to the host via pseudo-terminals or to other devices via
serial ports as shown in Figure \ref{fig:gezel_ipblock}.

\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \node(gezel){GEZEL};
    \node(ipblock)[left=of gezel]{ipblock};

    \draw[<->] (gezel) -- (ipblock);
  \end{tikzpicture}
  \caption{GEZEL and ipblock}
  \label{fig:gezel_ipblock}
\end{figure}

The ipblock terminal is based on POSIX termios functionality so this
limits the simulator availability to POSIX systems. This ipblock was
originally aimed to be connected to gplatform's 8051 simulator but the
simulator core itself does not allow easy connections of an external
serial port. The decision was made to make a transceiver ipblock that
receives and transmits arbitrary numbers. Such a transceiver allows
easier designs inside GEZEL as there is no need to parse the stream
input.

\begin{lstlisting}[language=GEZEL]
ipblock my_term(in tx    : ns(1024);
                out rx   : ns(1024);
                in sr    : ns(2);
                out done : ns(1)) {
  iptype "transceiver";
}
\end{lstlisting}

Output to the ``outside'' world is given to the \emph{tx} pin (marked
here as in because the directions are relative to the ipblock). Input
from the ``outside'' world is taken from the \emph{rx} pin. The
``outside'' world denotes the process running outside of the
simulation that is connected via a terminal. The \emph{sr} specifies
the operation: $0$ for no operation, $1$ for sending, $2$ for
receiving. After the operation is done, the ipblock sets the
\emph{done} pin to $1$.

\subsection{Modular exponentiation}

GEZEL supports modular operations (addition, subtraction and
multiplication) via two binary operations, first the normal operation
and then taking the modulus. GEZEZ, however, did not support modular
exponentiation. It was either necessary to implement it in GEZEL code
or to extend GEZEL with the exponentiation operation. As the
exponentiation is much easier to realize in C++ code the decision was
made to extend GEZEL. This required rewriting the grammar, adding a
new operation and adding run-time emitting of the code for the
exponentiation.

The syntax of the modular exponentiation is:
\begin{lstlisting}
y = g ** x % p;
\end{lstlisting}

GEZEL supports unary, binary and ternary operations. Since GEZEL uses
the GMP library, the modular exponentiation needed to be a ternary
operation. The GMP library supports only modular exponentiation when
using arbitrary precision integers. This required a significant change
in the core but the advantages of a having a well tested and working
implementation of modular exponentiation outweighed the disadvantages.

\section{Extensions to CACE Zero Knowledge Compiler}
\label{sec:cace_extensions}

\subsection{Terminal Functionality}

The C support library has been extended with terminal
functionality. Terminal communication is simpler as it can be
implemented over an RS232 communication protocol. The file adding this
functionality is comm-term.c. Changes to the adapter comm.c were also
needed to forward the requests. After implementing the terminal
functionality, a test was carried between a generated prover and a
verifier and later on between the generated entities and dummy
entities written in GEZEL. This allowed for cross-testing the terminal
implementation of GEZEL as well.

\subsection{Lower End}

Extensions to the CACE frameworks are presented in Figure
\ref{fig:custom_framework_workflow}. LLVM has already proven itself in
many areas so the choice was made to transform the lower-level PIL
into LLVM IR. LLVM IR can then be used as a starting point for
multiple targets. 

The custom part is further explained in Figure
\ref{fig:custom_llvm_workflow}. The input file is processed by a PIL
fronted which generates LLVM IR code which can either be fed to
existing optimizers or a custom one can be written. The resulting IR
can then either be fed to a backend for an existing architecture or a
new back-end can be made (such as 8051 and GEZEL in this case).

\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \tikzstyle{lang}=[rectangle,draw=black,thin,font=\tiny,inner
    sep=0pt, align=center,minimum width=2.7cm,minimum height=2.2em]

    \tikzstyle{txt}=[font=\tiny]

    \node[lang](llvm_opt){LLVM \\ Optimizer};
    \node[lang](ppc_back)[right=1 cm of llvm_opt]{LLVM \\ PowerPC Backend};
    \node[txt](ppc)[right=of ppc_back]{PowerPC};
    \node[lang](x86_back)[above of=ppc_back]{LLVM \\ x86 Backend};
    \node[txt](x86)[right=of x86_back]{x86};
    \node[lang](arm_back)[below of=ppc_back]{LLVM \\ ARM Backend};
    \node[txt](arm)[right=of arm_back]{ARM};
    \node[lang,added](gezel_back)[below of=arm_back]{Custom \\ GEZEL Backend};
    \node[txt](gezel)[right=of gezel_back]{GEZEL};

    \node[lang,added](pil_front)[left=1 cm of llvm_opt]{PIL \\ Frontend};
    \node[txt](pil)[left=of pil_front]{PIL};

    \draw[->] (pil_front.east) -- (llvm_opt.west);

    \draw[->] (llvm_opt.east) -- (x86_back.west);
    \draw[->] (llvm_opt.east) -- (ppc_back.west);
    \draw[->] (llvm_opt.east) -- (arm_back.west);

    \draw[->] (x86_back) -- (x86);
    \draw[->] (ppc_back) -- (ppc);
    \draw[->] (arm_back) -- (arm);

    \draw[->] (pil) -- (pil_front);

    \draw[->] (llvm_opt.east) -- (gezel_back.west);

    \draw[->] (gezel_back) -- (gezel);
  \end{tikzpicture}
  \caption{LLVM custom workflow (changes highlighted)}
  \label{fig:custom_llvm_workflow}
\end{figure}

\newpage

\subsection{Extensions to PIL}
\label{subsec:pil_extensions}

\subsubsection{Multiple Blocks}

The base PIL language supported only a Prover and a Verifier block
besides the Common block. This makes it impossible to implement a
multiparty protocol\footnote{In the cryptographic world, this means
  three or more parties}. As this is simply a relaxation of the rules,
the change to the grammar and the code generator was trivial.

\subsubsection{Compile Time/Constant Expressions}

Compile time/constant expressions were needed to allow more advanced
specifications of protocols. The PIL language does not specify
constant expressions as parameters of variables. When designing more
complex protocols where parameters of variables have to be adjusted,
one needs constant expressions. The benefit of constant expressions
can be seen from the following example:
\begin{lstlisting}[language=PIL]
Common (
  Z l_n = 1024;
  Z l_f = 160;
  Z l_e = 410;
  Z l_e_1 = 120;
  Z l_v = 1512;
  Z l_phi = 80;
  Z l_H = 160;
  Prime(l_n) n = 17
) {

}

Smartcard (
  Zmod*(n) p, q;
  Int(l_f + l_phi + l_H) f
) {
  Zmod*(n) S, _Z, R;
  Int(l_n + l_phi) v_1;
  Int(l_v) v;
  ...
\end{lstlisting}
Without constant expressions, one would need to recompute the values
manually and enter them every time a change was desired. This
re-computation and reentering is prone to errors and as such
undesirable when designing a crypto framework. The grammar change to
allow constant expressions is very simple
\begin{lstlisting}[language=grammar, keywords={group, expr}]
group	:	('Zmod+'|'Zmod*') '(' expr ')' 
	|	'Prime' '(' expr ')'
	|	'Int' '(' expr ')'
	|	'Z'
	;
\end{lstlisting}
This change allows it only syntactically so some semantic processing
will be needed to ensure that they are constant expressions which can
be evaluated at compile time by the compiler.

\subsubsection{Type Inference}

To be able to check for correctness, one needs to determine the resulting
type of a certain expression. This can be also used to allow one to omit
a type declaration. The following example illustrates this:
\begin{lstlisting}[language=PIL]
Zmod*(p) b;
x := Random(Int(80));
a := b^x;
\end{lstlisting}
For the case of variable x, its type can be inferred as Int(80) since
the Random function can only return a random value of the provided
type. For the case of variable a, the situation is a bit more
complex. However, if the operation of exponentiation is defined as
applying the multiplication operation many times, then the type of a
can only be Zmod*(p) by the definition of the multiplicative modular
residue group. A similar case can be made when multiplying an element
of the additive modular residue group with an integer. This means that
the type inference is well defined for any acceptable operation in
PIL.

\section{PIL Front-end}
\label{sec:pil_frontend}

PIL frontend flow is depicted in Figure \ref{fig:pil_frontend_flow}.  An
input PIL is read by the Lexer producing input for the Parser. The
Parser reads these and generates an abstract syntax tree (AST) which
is fed to the Codegen tree-walker that generates the code (in the form
of LLVM IR). Both the lexer grammar and the parser grammar are
specified in the file pil.g. The tree-walker and the code generator is
specified in the file codegen.g.

\begin{figure}[hb!]
  \centering
  \subfloat{
  \begin{tikzpicture}[>=stealth]
    \tikzstyle{edge from parent}=[draw,->]

    \Tree[.\node[language](parser_g){pil.g};
      [.\node[compiler](antlr){ANTLR};
        [.\node[compiler](lexer){Lexer};]
        [.\node[compiler](parser){Parser};]
      ]
    ]
  \end{tikzpicture}
  } \qquad
  \subfloat{
  \begin{tikzpicture}[>=stealth]
    \tikzstyle{edge from parent}=[draw,->]

    \Tree[.\node[language](tree_g){codegen.g};
      [.\node[compiler](antlr2){ANTLR};
        [.\node[compiler](walker){Codegen};]
      ]
    ]
  \end{tikzpicture}
  }
  \caption{Lexer, parser and tree walker generation}
\end{figure}

\begin{figure}[hb!]  
  \begin{tikzpicture}[>=stealth]
    \node[language] (pil) {PIL};
    \node[compiler] (lexer) [right=of pil] {Lexer};
    \node[compiler] (parser) [right=of lexer] {Parser};
    \node[language] (ast) [right=of parser] {AST};
    \node[compiler] (codegen) [right=of ast] {Codegen};
    \node[language] (llvm_ir) [right=of codegen] {LLVM IR};

    \draw[->] (pil) -- (lexer);
    \draw[->] (lexer) -- (parser);
    \draw[->] (parser) -- (ast);
    \draw[->] (ast) -- (codegen);
    \draw[->] (codegen) -- (llvm_ir);
  \end{tikzpicture}
  \caption{PIL frontend flow}
  \label{fig:pil_frontend_flow}
\end{figure}

The code generation process generates one module per block. The Common
block module is augmented with the functions provided by the VM.
Every other block except the Common block gets a Common block linked
in. This process is depicted in Figure \ref{fig:linker}.

\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \node[language] (block) {Block};
    \node[compiler] (linker) [right=of block] {Linker};
    \node[language] (common) [above=of linker] {Common};
    \node[language] (module) [right=of linker] {Module};

    \draw[->] (block) -- (linker);
    \draw[->] (common) -- (linker);
    \draw[->] (linker) -- (module);
  \end{tikzpicture}
  \caption{Linker}
  \label{fig:linker}
\end{figure}

The private parameters as well as global variables are transformed as
LLVM IR global variables, the private parameters being constant in
this case. Each function of a block gets transformed as an LLVM IR
function with its input and output arguments transformed as such. LLVM
allows for multiple return values so this is used as well when there
are multiple return values.

LLVM's constant folder is used to evaluate constant time expressions
to simplify them into a constant value.

When dealing with group arithmetic, which LLVM does not support, there are
two possibilities:
\begin{enumerate}
\item extend the LLVM's type system to include group types - this
  involves editing the core LLVM files, adding a new DerivedType,
  changing the bit-code format and adding changes to the LLVM parser
  (both binary and textual). Also, binary operations have to be redefined
  to support these new types \cite{extending_llvm}.
\item use a simpler, existing LLVM type and make the compiler do the
  extra work - the IntegerType of LLVM can be used for arbitrarily sized
  integers.
\end{enumerate}

The first option allows for preserving the information about the
modular residue groups to the lowest level. The transformation to a
primitive type supported by the target architecture happens only at a
later stage. Or, if the target architecture supports modular residue
groups, there is no need for a transformation, only a translation.
This allows for a code that is more verifiable and more secure as the
properties are kept to the lowest possible levels (no exploits can be
made under secure starting conditions). The disadvantage of this
method is the changes that need to be introduced at the heart of such
a complex framework as LLVM is. This is both time consuming and error
prone. Also, it becomes more difficult to track newer versions of LLVM
(with possibly better optimizations and more features) if the changes
are not integrated back into the main project. This method also breaks
compatibility with existing LLVM applications, so a specially patched
version of LLVM needs to be distributed.

The second option is easier as the changes to the compiler are only
local and do not break compatibility with existing LLVM applications.
It is also easier as there is no hard work associated with changing
the core of LLVM. More work has to be assigned to the compiler because
it needs to track types and apply the appropriate operations in the
case of modular residue groups. The burden is also on the back-end if
an architecture supports modular residue groups since it now has to
regenerate the lost information. An example of such an architecture
can be an automatic verifier whose job is now made more complex. This
re-generation of information might also not be always completely
accurate.

The first approach was attempted first but was deemed too time
consuming and error prone. Also it would require some standardization
with LLVM upstream to allow future tracking. The second approach was
chosen as the one to base the work on. The types are backed by an
IntegerType of the appropriate length in the LLVM IR. Type inference
was used to deduce the resulting type of an operation. The operation
was then sent to appropriate 3 argument operations with the first 2
being the operands as integers and the third operand being the
modulus.

\subsection{Type Alias}

Type aliases are evaluated and substituted by the parser. The
reasoning behind them can go both ways. Although they are more
semantic than syntactic since they convey information (as a creation
of another type), they are simple enough to be evaluated and
substituted by the parser. This can be paralleled with a pre-processor
in languages which support it. The rule includes a syntactic predicate
to test if it is a declaration of an alias or an alias is referenced.
\begin{lstlisting}[language=C++, keywords={ID, group, interval, alias}]
alias :
      (ID '=' )=> ID! '='! (
        group { aliases[(const char*)$ID.text->chars] = $group.tree; } 
      | interval { aliases[(const char*)$ID.text->chars] = $interval.tree; }
      )
      |	ID -> { aliases[(const char*)$ID.text->chars] }
      ;
\end{lstlisting}
If the syntactic predicate matches, the alias is stored in a hashmap,
otherwise the hashmap is searched for the alias. This syntactic
predicate effectively implements a 1 symbol look-ahead.

\subsection{Type system}

The compiler has to keep track of the types. A simple type system was
designed that allows both numbers and modular residues. The UML of
this type system is given in Figure \ref{fig:type_system}.

\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}
    \tikzumlset{font=\tiny}

    \umlclass{NumberT}{
      \# width : const uint64\_t
    }{
      \umlvirt{+ getType() : llvm::Type} \\ 
      \umlvirt{+ getBitWidth() : uint64\_t}
    }
    \umlclass[y=-3]{GroupT}{
      \# modulus : const llvm::APInt
    }{
      + getModulus() : llvm::APInt \\
      + getModulusConstant() : llvm::ConstantInt
    }
    \umlinherit{NumberT}{GroupT}
  \end{tikzpicture}
  \caption{Type system UML}
  \label{fig:type_system}
\end{figure}

NumberT represents any general number type that can occur (types Int
and Z), while GroupT represents group types (types Zmod+ and Zmod*). The
getType() method returns the backing type in the LLVM IR (IntegerType
of the appropriate length). The modulus of the GroupT is stored as an
LLVM arbitrary precision integer (llvm::APInt) and it can be fetched
by using the getter getModulus(). getModulusConstant() is a helper
method that wraps it in a ConstantInt which LLVM can then use for
constant folding.

\subsection{Type Inference}

The easiest approach for determining a resulting type from two
operands is to use double dispatch and code each of the resulting
functions.  There were some attempts at bringing multiple dispatch to
C++ but this has still not been realized \cite{c++_multi_methods}.

A common way to simulate it in C++ is to use a visitor pattern. The
base interface or the base class defines accept methods. The visitor
then calls each of the classes dispatching itself as a parameter. Here
is how it is done with GroupT and NumberT:

\begin{lstlisting}[language=C++]
class NumberT {
  ...
  virtual NumberT *addWithSubFrom(const NumberT *first);
  ...
  virtual NumberT *operator+(const NumberT &other) const {
    return other.addWithSubFrom(this);
  }
};

class GroupT : public NumberT {
  ...
};
\end{lstlisting}

Here the addWithSubFrom method is the accept method.  The same was
done for each of the operators. When the compiler encouters a + b
where a and b have NumberT as a base class, it will call the
\textbf{operator+}. This call happens through a \emph{vtable} so the
\emph{this} pointer always points to the accurate type for the first
operand. By calling the addWithSubFrom method, another \emph{vtable}
dispatch happens and withing the addWithSubFrom \textbf{this} points
to the accurate type for the second operand \cite{Eckel}. Since both
of the operands are now correctly resolved it remains to return the
resulting type of the operation. Extending the type inference simply
involves writing a function for each combination of the operand
types. This is somewhat easier than using Run Time Type Information
and writing an if-then-else or similar for each of the cases.

Since GroupT derives from NumberT the operator calls will remain this
way unless overridden. The accept methods will need to be overridden
as they provide the custom logic for each of the combinations.

The classes NumberT and GroupT also define methods for creating an
LLVM instruction for addition, subtraction, multiplication and
exponentiation. These function as adapters and make the process of
code-generation uniform. Here is an excerpt from the code generation:
\begin{lstlisting}[language=C++, morekeywords=expr]
expr[const char *id] returns [Value *value, NumberT *type]
	:	...
	|	^('+' lhs=expr[$id] rhs=expr[$id]) {
                  $type = *$lhs.type + *$rhs.type;
                  $value = $type->createAdd($lhs.value, $rhs.value);
                }
	|	...
\end{lstlisting}

\section{GEZEL Back-end}

The GEZEL Back-end extracts the Data Flow Graph (DFG) and implements
one GEZEL datapath per round (Figure \ref{fig:gezel_backend}). The
Control Flow Graph (CFG) is maximally unrolled such that the DFG
within a round is purely combinatorial. Each invocation of addition,
multiplication, exponentiation requires a new instantiation of an
adder, multiplier, exponentiator, respectively.


\begin{figure}[hb!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \node[language, inner sep=0.5cm](round0){Round0};
    \node[language, inner sep=0.5cm](round1)[below=of round0]{Round1};
    \node[language, inner sep=0.5cm](round2)[below=of round1]{Round2};

    \node[language, inner sep=0.5cm](regs)[above=of round0]{Registers};

    \node[language, inner sep=0.5cm](mux_in)[left=2cm of round1]{Input bus};

    \node[language, inner sep=0.5cm](mux_out)[right=2cm of round1]{Output multiplexer};

    \draw[->](round0.east) -- (mux_out.west);
    \draw[->](round1.east) -- (mux_out.west);
    \draw[->](round2.east) -- (mux_out.west);
    \draw[->](mux_in.east) -- (round0.west);
    \draw[->](mux_in.east) -- (round1.west);
    \draw[->](mux_in.east) -- (round2.west);
    \draw[->](regs.west) -- (mux_in.north |- regs.west) -- (mux_in.north);
    \draw[->](mux_out.north) -- (mux_out.north |- regs.east) -- (regs.east);
  \end{tikzpicture}
  \caption{Generated GEZEL model of Schnorr's protocol}
  \label{fig:gezel_backend}
\end{figure}

The choice of maximally unrolling the CFG was made as it already
allows architecture exploration at one end of the spectrum while the
other is provided by the software implementation. The implementations
in the middle of the spectrum should be derived from the closer side
by rolling the CFG or adding more to the software. Such techniques
require more complex signaling and scheduling and, as time was
limited, were not taken.

The memory accesses are converted to register accesses and each
datapath gets access to each of the registers. To prevent name collisions
register inputs and outputs are decorated as shown below:

\filbreak

\begin{lstlisting}[language=GEZEL]
dp round1(in  r_r_1_i : ns(160); out r_r_1_o : ns(160);
          in  r_s_1_i : ns(160); out r_s_1_o : ns(160);
          out _t_1 : ns(1024)) {
          
  sig s_r_1 : ns(1024);
  sig s_t_1 : ns(1024);

  use random1(11, s_r_1);
  use modexp1(3, s_r_1, 23, s_t_1);
  
  always {
    r_r_1_o = s_r_1;
    r_s_1_o = r_s_1_i;

    _t_1 = s_t_1;
  }
}
\end{lstlisting}

The example is generated from the Schnorr's Identification Protocol
PIL (see example of \ref{subsec:pil}). Each register is prefixed with
\emph{r} and suffixed with \emph{i} if referring to the input from the
register or \emph{o} if referring to the output from the register. The
internal signals within a datapath are prefixed with \emph{s}.

%%% Local Variables: 
%%% TeX-PDF-mode: t
%%% TeX-master: "thesis"
%%% End: 
