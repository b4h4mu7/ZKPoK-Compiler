\chapter{Custom framework}

\begin{figure}[hbt!]
  \centering
    \begin{tikzpicture}[>=stealth,level distance=1.5cm, font=\tiny]
    \tikzstyle{edge from parent}=[draw,->]
    \tikzset{every leaf node/.style={anchor=center}}

    \Tree [.\node[language](psl){Protocol \\ Specification \\ Language (PSL)};
      [.\node[compiler](pc){Protocol \\ Compiler};
        [.\node[language](pil){Protocol \\ Implementation \\ Language (PIL)};
          [.\node[compiler,added](llvm){LLVM};
            \node[language,added](asm){x86 \\ x86-64 \\ MIPS \\ARM \\ \emph{8051}};
            \node[language,added](gezel){GEZEL \\ VHDL \\ Verilog};
          ]
          [.\node[compiler](c){C}; \node[language](code){Code};]
          [.\node[compiler](latex){\LaTeX}; \node[language](doc){Documentation};]
        ]
      ]
    ]

    \node[compiler] (pvt)         [right=of pc,anchor=west]          {Protocol \\ Verification \\ Toolbox}
    child {node[language] {Proof of \\ Soundness}};

    \node[compiler] (sigma) [left=of pil.north west,anchor=center] {$\Sigma 2 N I Z K$};
    \node[compiler] (cost) [left=of pil.south west,anchor=center] {Costs};

    \draw[<->] (sigma) -- (pil);
    \draw[<->] (cost) -- (pil);

    \draw[->] (psl) -- (pvt);
    \draw[->] (pil) -- (pvt);
  \end{tikzpicture}
  \caption{Custom framework (extensions to CACE Zero Knowledge
    Compiler highlighted)}
  \label{fig:custom_framework_workflow}
\end{figure}

\section{Motivation}

\subsection{Domain specific language}

C is not a very safe language for cryptography applications. This
can be seen already from the goal of C which was to design a
``portable'' assembler. The design choice was not to sacrifice
efficiency so some things are intentionally left undefined or
unspecified in the standard. It is left to the end compilers to
implement it as they chose. The usual choice is just to implement in
the most efficient way for the target platform. For example, the
following code uses this unspecified behavior:

\begin{lstlisting}[language=C]
#include <stdio.h>

int b(void) { puts("3"); return 3; }
int c(void) { puts("4"); return 4; }

int main(void)
{
  int a = b() + c();
  printf("%d\n", a);

  return 0;
}
\end{lstlisting}

The evaluation order is unspecified so depending on the compiler and
the platform, the output may be 3, 4, 7 or 4, 3, 7. The crypto world
should not have undefined nor unspecified behaviors so this is why a
domain specific language is a key point of this thesis.

\subsection{Extending CACE Zero Knowledge Compiler}

The C code generated by CACE uses GNU GMP which is a multi-precision
arithmetic library. This library is tailored for desktop computers and
is not well suited for small embedded devices. Granted, it is possible
to plug in a custom multi-precision library but it is impossible,
error prone and inefficient to design a library for every imaginable
device. The fact that this compiler already uses its own domain
specific language makes it a useful candidate for extension. It is
possible to use LLVM as a low level language and assure portability
and type strictness.

ZKPDL also has its own domain specific language. The approach taken,
with creating an interpreter is considered too inefficient for
embedded devices.

As the CACE project is deemed more suited for the task, the custom
framework will extend CACE. The custom framework will aim to be a
compiler that can produce:
\begin{itemize}
\item Interpreted code
\item JIT (Just In Time) compiled code
\item Compiled code
\end{itemize}
Also, since its intermediate form is of the SSA (Static Single
Assignment) form, it is believed that more aggressive optimizations
are possible. Since the DFG (Data Flow Graph) can be easily extracted
from the SSA form, hardware realizations are also made possible.

\filbreak

\section{Extensions to GEZEL}

A modification to GEZEL was needed to allow interactivity. A terminal
ipblock was made during the course of this thesis. This ipblock allows
connecting to the host via pseudo-terminals or to other devices via
serial ports as shown in Figure \ref{fig:gezel_ipblock}.

\begin{figure}[hbt!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \node(gezel){GEZEL};
    \node(ipblock)[left=of gezel]{ipblock};

    \draw[<->] (gezel) -- (ipblock);
  \end{tikzpicture}
  \caption{GEZEL and ipblock}
  \label{fig:gezel_ipblock}
\end{figure}

The ipblock terminal is based on POSIX termios functionality so this
limits the simulator availability to POSIX systems.

\section{Extensions to CACE Zero Knowledge Compiler}

\subsection{Terminal functionality}

The C support library has been extended with terminal
functionality. Terminal communication is simpler as it can be
implemented over an RS232 communication protocol. The file adding this
functionality is comm-term.c. Changes to the adapter comm.c were also
needed to forward the requests. After implementing the terminal
functionality, a test was carried between a generated prover and a
verifier and later on between the generated entities and dummy
entities written in GEZEL. This allowed for cross-testing the terminal
implementation of GEZEL as well.

\subsection{Lower end}

Extensions to the CACE frameworks are presented in Figure
\ref{fig:custom_framework_workflow}. LLVM has already proven itself in
many areas so the choice was made to transform the lower-level PIL
into LLVM IR. LLVM IR can then be used as a starting point for
multiple targets. 

The custom part is further explained in Figure
\ref{fig:custom_llvm_workflow}. The input file is processed by a PIL
fronted which generates LLVM IR code which can be fed to either
existing optimizers or a custom one can be written. The resulting IR
can then be fed to a backend for an existing architecture or a new
back-end can be made (such as 8051 and GEZEL in this case).

\begin{figure}[hbt!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \tikzstyle{lang}=[rectangle,draw=black,thin,font=\tiny,inner
    sep=0pt, align=center,minimum width=2.7cm,minimum height=2.2em]

    \tikzstyle{txt}=[font=\tiny]

    \node[lang](llvm_opt){LLVM \\ Optimizer};
    \node[lang](ppc_back)[right=1 cm of llvm_opt]{LLVM \\ PowerPC Backend};
    \node[txt](ppc)[right=of ppc_back]{PowerPC};
    \node[lang](x86_back)[above of=ppc_back]{LLVM \\ x86 Backend};
    \node[txt](x86)[right=of x86_back]{x86};
    \node[lang](arm_back)[below of=ppc_back]{LLVM \\ ARM Backend};
    \node[txt](arm)[right=of arm_back]{ARM};
    \node[lang,added](8051_back)[below of=arm_back]{Custom \\ 8051 Backend};
    \node[txt](8051)[right=of 8051_back]{8051};
    \node[lang,added](gezel_back)[below of=8051_back]{Custom \\ GEZEL Backend};
    \node[txt](gezel)[right=of gezel_back]{GEZEL};

    \node[lang,added](pil_front)[left=1 cm of llvm_opt]{PIL \\ Frontend};
    \node[txt](pil)[left=of gcc_front]{PIL};

    \draw[->] (gcc_front.east) -- (llvm_opt.west);

    \draw[->] (llvm_opt.east) -- (x86_back.west);
    \draw[->] (llvm_opt.east) -- (ppc_back.west);
    \draw[->] (llvm_opt.east) -- (arm_back.west);

    \draw[->] (x86_back) -- (x86);
    \draw[->] (ppc_back) -- (ppc);
    \draw[->] (arm_back) -- (arm);

    \draw[->] (pil) -- (pil_front);

    \draw[->] (llvm_opt.east) -- (8051_back.west);
    \draw[->] (llvm_opt.east) -- (gezel_back.west);

    \draw[->] (8051_back) -- (8051);
    \draw[->] (gezel_back) -- (gezel);
  \end{tikzpicture}
  \caption{LLVM custom workflow (changes highlighted)}
  \label{fig:custom_llvm_workflow}
\end{figure}

\newpage

\subsection{Extensions to PIL}

\subsubsection{Multiple blocks}

The base PIL language supported only a Prover and a Verifier block
besides the Common block. This makes it impossible to implement a more
than 2 parties protocol. As this is simply a relaxation of the rules,
the change to support 3 and more parties was trivial.

\subsubsection{Compile time/constant expressions}

Compile time/constant expressions were needed to allow more advanced
specifications of protocols. The PIL language does not specify
constant expressions as parameters of variables. When designing more
complex protocol where parameters of variables have to be adjusted,
one needs constant expressions. The benefit of constant expressions
can be seen from the following example:
\begin{lstlisting}[language=PIL]
Common (
  Z l_n = 1024;
  Z l_f = 160;
  Z l_e = 410;
  Z l_e_1 = 120;
  Z l_v = 1512;
  Z l_phi = 80;
  Z l_H = 160;
  Prime(l_n) n = 17
) {

}

Smartcard (
  Zmod*(n) p, q;
  Int(l_f + l_phi + l_H) f
) {
  Zmod*(n) S, _Z, R;
  Int(l_n + l_phi) v_1;
  Int(l_v) v;
  ...
\end{lstlisting}
Without constant expressions, one would need to recompute the values
manually and enter them every time a change was desired. This
re-computation and reentering is prone to errors and as such
undesirable when designing a crypto framework. The grammar change to
allow constant expressions is very simple
\begin{lstlisting}[language=grammar, keywords={group, expr}]
group	:	('Zmod+'|'Zmod*') '(' expr ')' 
	|	'Prime' '(' expr ')'
	|	'Int' '(' expr ')'
	|	'Z'
	;
\end{lstlisting}
This change allows it only syntactically so some semantic processing
will be needed to ensure that they are constant expressions which can
be evaluated at compile time by the compiler.

\subsubsection{Type inference}

To be able to check for correctness, one needs to determine the resulting
type of a certain expression. This can be also used to allow one to omit
a type declaration. The following example illustrates this:
\begin{lstlisting}[language=PIL]
Zmod*(p) b;
x := Random(Int(80));
a := b^x;
\end{lstlisting}
For the case of variable x, it's type can be inferred as Int(80) since
the Random function can only return a random value of the provided
type. For the case of variable a, the situation is a bit more
complex. However, if the operation of exponentiation is defined as
applying the multiplication operation many times, then the type of a
can only be Zmod*(p) by the definition of the multiplicative modular
residue group. A similar case can be made when multiplying an element
of the additive modular residue group with an integer. This means that
the type inference is well defined for any acceptable operation in
PIL.

\section{PIL front-end}
\label{sec:pil_frontend}

PIL frontend flow is depicted in Figure \ref{fig:pil_frontend_flow}.  An
input PIL is read by the Lexer producing input for the Parser. The
Parser reads these and generates an abstract syntax tree (AST) which
is fed to the Codegen tree-walker that generates the code (in the form
of LLVM IR). Both the lexer grammar and the parser grammar are
specified in the file pil.g. The tree-walker and the code generator is
specified in the file codegen.g.

\begin{figure}[hbt!]
  \centering
  \begin{tikzpicture}[>=stealth, font=\tiny]
    \tikzstyle{edge from parent}=[draw,->]

    \Tree[.\node[language](parser_g){pil.g};
      [.\node[compiler](antlr){ANTLR};
        [.\node[compiler](lexer){Lexer};]
        [.\node[compiler](parser){Parser};]
      ]
    ]

    \node[language](AST)[right=of parser]{AST};

    \Tree[.\node[language](tree_g)[right=2 cm of parser_g]{codegen.g};
      [.\node[compiler](antlr2)[right=2 cm of antlr]{ANTLR};
        [.\node[compiler](walker)[right=of AST]{Tree parser};]
      ]
    ]
  \end{tikzpicture}
  \caption{ANTLR workflow}
  \label{fig:antlr_workflow}
\end{figure}

\begin{figure}[hbt!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \node[language] (pil) {PIL};
    \node[compiler] (lexer) [right=of pil] {Lexer};
    \node[compiler] (parser) [right=of lexer] {Parser};
    \node[language] (ast) [right=of parser] {AST};
    \node[compiler] (codegen) [right=of ast] {Codegen};
    \node[language] (llvm_ir) [right=of codegen] {LLVM IR};

    \draw[->] (pil) -- (lexer);
    \draw[->] (lexer) -- (parser);
    \draw[->] (parser) -- (ast);
    \draw[->] (ast) -- (codegen);
    \draw[->] (codegen) -- (llvm_ir);
  \end{tikzpicture}
  \caption{PIL frontend flow}
  \label{fig:pil_frontend_flow}
\end{figure}

\filbreak

The code generation process generates one module per block. The Common
block module is augmented with the functions provided by the VM.
Every other block except the Common block gets a Common block linked
in. This process is depicted in Figure \ref{fig:linker}.

\begin{figure}[hbt!]
  \centering
  \begin{tikzpicture}[>=stealth]
    \node[language] (block) {Block};
    \node[compiler] (linker) [right=of block] {Linker};
    \node[language] (common) [above=of linker] {Common};
    \node[language] (module) [right=of linker] {Module};

    \draw[->] (block) -- (linker);
    \draw[->] (common) -- (linker);
    \draw[->] (linker) -- (module);
  \end{tikzpicture}
  \caption{Linker}
  \label{fig:linker}
\end{figure}

The private parameters as well as global variables are transformed as
LLVM IR global variables, the private parameters being constant in
this case. Each function of a block gets transformed as an LLVM IR
function with its input and output arguments transformed as such. LLVM
allows for multiple return values so this is used as well when there
are multiple return values.

LLVM's constant folder is used to evaluate constant time expressions
to simplify them into a constant value.

When dealing with group arithmetic which LLVM does not support, there are
two possibilities:
\begin{enumerate}
\item extend the LLVM's type system to include group types - this
  involves editing the core LLVM files, adding a new DerivedType,
  changing the bit-code format and adding changes to the LLVM parser
  (both binary and textual). Also, binary operations have to redefined
  to support these new types \cite{extending_llvm}.
\item use a simpler, existing LLVM type and make the compiler do the
  extra work - the IntegerType of LLVM can be used for arbitrarily sized
  integers.
\end{enumerate}

The first option allows for preserving the information about the
modular residue groups to the lowest level. The transformation to a
primitive type supported by the target architecture happens only at a
later stage. Or, if the target architecture supports modular residue
groups, there is no need for a transformation, only a translation.
This allows for a code that is more verifiable and more secure as the
properties are kept to the lowest possible levels (no exploits can be
made under secure starting conditions). The disadvantage of this
method is the changes that need to be introduced at the heart of such
a complex framework as LLVM is. This is both time consuming and error
prone. Also, it becomes more difficult to track newer versions of LLVM
(with possibly better optimizations and more features) if the changes
are not integrated back into the main project. This method also breaks
compatibility with existing LLVM applications so a specially patched
version of LLVM needs to be distributed.

The second option is easier as the changes to the compiler are only
local and do not break compatibility with existing LLVM applications.
It is also easier as there is no hard work associated with changing
the core of LLVM. More work has to be assigned to the compiler because
it needs to track types and apply the appropriate operations in the
case of modular residue groups. The burden is also on the back-end if
an architecture supports modular residue groups since it now has to
regenerate the lost information. An example of such an architecture
can be an automatic verifier whose job is now made more complex. This
re-generation of information might also not be always completely
accurate.

The first approach was attempted first but was deemed too time
consuming and error prone. Also it would require some standardization
with LLVM upstream to allow future tracking. The second approach was
chosen as the one to base the work on. Type inference was used to
deduce the resulting type of an operation. The operation was then sent
to appropriate 3 argument operations with the first 2 being the
operands as integers and the 3rd operand being the modulus.

\subsection{Type alias}

Type aliases are evaluated and substituted by the parser. The
reasoning behind them can go both ways. Although they are more
semantic than syntactic since they convey information (as a creation
of another type), they are simple enough to be evaluated and
substituted by the parser. This can be paralleled with a pre-processor
in languages which support it.

\subsection{Type inference}

The easiest approach for determining a resulting type from two
operands is to use double dispatch and code each of the resulting
functions.  There were some attempts at bringing multiple dispatch to
C++ but this has still not been realized \cite{c++_multi_methods}.

A common way to simulate it in C++ is to use a visitor pattern. The base
interface or the base class defines accept methods. The visitor then calls
each of the classes dispatching itself as a parameter. Here is how it done
with GroupT and NumberT:

\begin{lstlisting}[language=C++]
  class NumberT {
  ...
  virtual NumberT *addWithSubFrom(const NumberT *first);
  ...
  virtual NumberT *operator+(const NumberT &other) const {
    return other.addWithSubFrom(this);
  }
}
\end{lstlisting}

Here the addWithSubFrom method is the accept method.  The same was
done for each of the operators. When the compiler encouters a + b
where a and b have NumberT as a base class, it will call the
operator+. This call happens through a vtable so the this pointer
always points to the accurate type for the first operand. By calling
the addWithSubFrom method, another vtable dispatch happens and now
this points to the accurate type for the second operand
\cite{Eckel}. Since both of the operands are now correctly resolved it
remains to return the resulting type of the operation. Extending the
type inference simply involves writing a function for each combination
of the operand types. This is somewhat easier than using Run Time Type
Information and writing an if-then-else or similar for each of the
cases.

Since GroupT derives from NumberT the operator calls will remain this
way unless overridden. The accept methods will need to be overridden
as they provide the custom logic for each of the combinations.

The classes NumberT and GroupT also define methods for creating an
LLVM instruction for addition, subtraction, multiplication and
exponentiation. These function as adapters and make the process of
code-generation uniform. Here is an excerpt from the code generation:
\begin{lstlisting}[language=C++, morekeywords=expr]
expr[const char *id] returns [Value *value, NumberT *type]
	:	...
	|	^('+' lhs=expr[$id] rhs=expr[$id]) {
                  $type = *$lhs.type + *$rhs.type;
                  $value = $type->createAdd($lhs.value, $rhs.value);
                }
\end{lstlisting}

\section{GEZEL back-end}

%%% Local Variables: 
%%% TeX-PDF-mode: t
%%% TeX-master: "thesis"
%%% End: 
